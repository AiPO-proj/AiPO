{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8caa707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartoszgawron/Desktop/aipo/AiPO/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from Tex2loc import Tex2loc\n",
    "from GeoGessr import GeoGuessCountryClassifier\n",
    "from tekstoinator import extract_text_from_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9825ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifier_model(model_path, device, num_classes):\n",
    "    model = GeoGuessCountryClassifier(num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa2538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return preprocess(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a4f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_country_on_video(video_path, model, device, label_encoder):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    predictions = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        \n",
    "        img_tensor = preprocess_frame(frame).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            predictions.append(pred.item())\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    most_common_idx = Counter(predictions).most_common(1)[0][0]\n",
    "    most_common_country = label_encoder.classes_[most_common_idx]\n",
    "    return most_common_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799ed84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    video_path = \"input/video.mp4\"\n",
    "    if len(sys.argv) > 1:\n",
    "        video_path = sys.argv[1]\n",
    "\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f\"Video file '{video_path}' nie istnieje!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Używany device: {device}\")\n",
    "\n",
    "    print(\"1. Wykonywanie OCR na video...\")\n",
    "    extracted_text_list = extract_text_from_video(video_path, frame_interval=25)\n",
    "    extracted_text = \" \".join(extracted_text_list)\n",
    "    print(f\"Znaleziony tekst (fragment): {extracted_text[:500]}...\")\n",
    "\n",
    "    print(\"2. Analiza lokalizacji z tekstu...\")\n",
    "    tex2loc = Tex2loc(device=device.type) \n",
    "    location_info = tex2loc.get_location_info(extracted_text)\n",
    "    print(f\"Lokalizacja z tekstu: {location_info}\")\n",
    "\n",
    "    print(\"3. Ładowanie klasyfikatora obrazów...\")\n",
    "   \n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    df = pd.read_csv(\"dataset/country_dataset.csv\", header=None, names=['country', 'lat', 'lon', 'local_path'])\n",
    "\n",
    "    counts = df['country'].value_counts()\n",
    "    valid_countries = counts[counts >= 2].index\n",
    "    df = df[df['country'].isin(valid_countries)]\n",
    "\n",
    "    countries = sorted(df['country'].unique())\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(countries)\n",
    "\n",
    "    model_path = \"best_model.pt\"\n",
    "    model = load_classifier_model(model_path, device, num_classes=len(le.classes_))\n",
    "    print(\"Model załadowany.\")\n",
    "\n",
    "    print(\"4. Predykcje na klatkach video...\")\n",
    "    most_common_country = predict_country_on_video(video_path, model, device, le)\n",
    "    print(f\"Najczęściej przewidywany kraj na video: {most_common_country}\")\n",
    "\n",
    "    print(\"\\n--- Podsumowanie ---\")\n",
    "    print(f\"Tekst OCR: {extracted_text[:500]}...\")\n",
    "    print(f\"Lokalizacja tekstowa: Miasto: {location_info['city']}, Kraj: {location_info['country']}, Kontynent: {location_info['continent']}\")\n",
    "    print(f\"Predykcja modelu na klatkach: {most_common_country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e812af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
