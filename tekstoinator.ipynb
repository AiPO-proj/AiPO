{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tekstoinator\n",
    "\n",
    "Robimy rzeczy nie dlatego, że są proste tylko dlatego, że są na zaliczenie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DETECTION_SIZE = (640, 640)\n",
    "\n",
    "def detect_and_annotate(image):\n",
    "    inputSize = (640, 640)\n",
    "    imc = image.copy()\n",
    "    image_scaled = cv2.resize(imc, (640, 640))\n",
    "    mean = (122.67891434, 116.66876762, 104.00698793)\n",
    "\n",
    "\n",
    "    image_processed = image_scaled.copy()\n",
    "\n",
    "    if image.shape[0] < 640 or image.shape[1] < 640:\n",
    "        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "        \n",
    "\n",
    "        Z = image_processed.reshape((-1,3))\n",
    "        Z = np.float32(Z)\n",
    "\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "        clusters_n = 4\n",
    "\n",
    "        ret,label,center=cv2.kmeans(Z,clusters_n,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n",
    "        center = np.uint8(center)\n",
    "        res = center[label.flatten()]\n",
    "        \n",
    "        image_processed = res.reshape((640, 640, 3))\n",
    "\n",
    "        image_processed = cv2.GaussianBlur(image_processed, (5,5), 2)\n",
    "\n",
    "    textDetectorDB50= cv2.dnn_TextDetectionModel_DB(\"./DB_TD500_resnet50.onnx\")\n",
    "    textDetectorDB50.setBinaryThreshold(0.2).setPolygonThreshold(0.4)\n",
    "    textDetectorDB50.setInputParams(1.0/255, inputSize, mean, False)\n",
    "    boxes, confidences = textDetectorDB50.detect(image_processed)\n",
    "\n",
    "\n",
    "    plausible_text = []\n",
    "\n",
    "    # Process all detected text\n",
    "    for idx, box in enumerate(boxes):\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(box)\n",
    "        \n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        if y < 0:\n",
    "            y = 0\n",
    "\n",
    "        plausible_text.append(image_scaled[y:y+h, x:x+w])\n",
    "    \n",
    "\n",
    "    return plausible_text\n",
    "\n",
    "# For finding road signs, hopefully, probably - not very reliably but good-enoughly\n",
    "# Why does grass have to be green? Well it is due to the composition of the Sun's light spectrum\n",
    "# The human eye is also adapted to receiving green for the same reason\n",
    "# This in turn is likely the reason designers chose green as a background for road signs\n",
    "# Truly the detection of road signs is made difficult by the laws of physics\n",
    "\n",
    "def find_green(image):\n",
    "    returned_images = []\n",
    "\n",
    "    green_lower = np.array([0, 60, 25])\n",
    "    green_upper = np.array([50, 255, 120])\n",
    "\n",
    "\n",
    "    small_kernel = np.ones((5,5),np.uint8)\n",
    "    big_kernel = np.ones((26,26),np.uint8)\n",
    "\n",
    "    \n",
    "\n",
    "    example_frame_masked = cv2.inRange(image, green_lower, green_upper)\n",
    "    example_frame_masked = cv2.morphologyEx(example_frame_masked, cv2.MORPH_OPEN, small_kernel)\n",
    "    example_frame_masked = cv2.dilate(example_frame_masked, big_kernel, iterations = 3)\n",
    "    example_frame_masked = cv2.morphologyEx(example_frame_masked, cv2.MORPH_CLOSE, small_kernel)\n",
    "\n",
    "    num_labels, labels_im = cv2.connectedComponents(example_frame_masked)\n",
    "    \n",
    "    for i in range(1,num_labels):\n",
    "        mask = (labels_im == i).astype(np.uint8)\n",
    "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(contours[0])\n",
    "        x_max, y_max = x + w, y + h\n",
    "\n",
    "        # Square good\n",
    "\n",
    "        if w > h:\n",
    "            if (image.shape[1] >= y + w):\n",
    "                h = w\n",
    "        elif h > w:\n",
    "            if (image.shape[0] >= x + h):\n",
    "                w = h\n",
    "\n",
    "        returned_images.append(image[y:y_max,x:x_max,  :])\n",
    "    \n",
    "    return returned_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "\n",
    "reader = easyocr.Reader(['en', 'pl', 'es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mov1 = cv2.VideoCapture('rondo.mp4')\n",
    "\n",
    "ret, frame = mov1.read()\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "\n",
    "\n",
    "\n",
    "found_text = []\n",
    "\n",
    "\n",
    "while (ret):\n",
    "    k += 1\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    \n",
    "\n",
    "    if k % 25 == 0:\n",
    "\n",
    "        # Try on the frame in general\n",
    "        found_text += reader.readtext(frame)\n",
    "\n",
    "        # Try to extract areas where text is more probable to be found\n",
    "        example_frames_annotated = []\n",
    "        example_frames_annotated += detect_and_annotate(frame)\n",
    "\n",
    "        found_green = find_green(frame)\n",
    "\n",
    "        for green_thing in found_green:\n",
    "            example_frames_annotated += detect_and_annotate(green_thing)\n",
    "\n",
    "        # Detect on those areas\n",
    "        for analyzed_frame in example_frames_annotated:\n",
    "            found_text += reader.readtext(analyzed_frame)\n",
    "\n",
    "    ret, frame = mov1.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_text = []\n",
    "\n",
    "for text_bit in found_text:\n",
    "    if text_bit[2] > 0.4 and len(text_bit[1]) > 3:\n",
    "        filtered_text.append(text_bit[1].upper())\n",
    "\n",
    "filtered_text = set(filtered_text)\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in example_frames_annotated:\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.title(\"Przykładowa klatka filmu\")\n",
    "    plt.imshow(frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obrazy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
